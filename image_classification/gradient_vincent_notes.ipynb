{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the Google Class Vincent Van...\n",
    "<h6>Vincent Gradient</h6>\n",
    "<h6>Numerical Stability</h6>\n",
    "Take 1B, add 10^-6 1M times so you are really adding 1, print result. \n",
    ".95 is not 1. \n",
    "<p>\n",
    "\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "If we replace the 1B number with 1 we see the error becomes very small. This is an argument for\n",
    "normalization of input values to be [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999999999177334e-07\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "for i in range(1):\n",
    "    a = a + 1e-6\n",
    "print(a - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h6>Gradients</h6>\n",
    "<img src=\"vincent_grad1.png\">\n",
    "<p>\n",
    "When computing the gradient for gradient descent, if computing the loss takes N floating point operations then computing\n",
    "the gradient takes 3x the number of operations. The loss function consists of computing the loss or floating \n",
    "point operations over all the training set examples. This dataset has to be big by definition for deep learning to\n",
    "work. \n",
    "</p>\n",
    "<p>\n",
    "To reduce the amount of computation we sample the training set instead of processing all the training examples. We saw\n",
    "this earlier on the derivative page. This sampling is called stochastic gradient descent. Not only are we going to sample\n",
    "but we are going to compute the average loss for a random set of the training data. Random is key. If the way we \n",
    "pick the samples is not random enough this no longer works. \n",
    "</p>\n",
    "<img src=\"vincent_grad2.png\">\n",
    "<p>\n",
    "This gradient is not in the same direction as the actual gradient calculated over all the training samples \n",
    "but it should be close and as we see above it eventually gets us close to the target. The key is we are going to do\n",
    "this many many times taking very very very small steps each time. This is important. The small steps help us converge. \n",
    "</p>\n",
    "<img src=\"vincent_grad3.png\">\n",
    "<img src=\"vincent_grad4.png\">\n",
    "<img src=\"vincent_grad5.png\">\n",
    "Learning rate decay field of active reasearch. Some like to reduce LR when loss reaches plateau, some like to reduce\n",
    "on each step. \n",
    "<img src=\"vincent_grad6.png\">\n",
    "<h6>Summary</h6>\n",
    "<li>Normalize input to 0 mean and equal variance. A random variable with samles will not have variation\n",
    "in the variance values for equal variance. Or a fixed small standard deviation. </li>\n",
    "<li>Learning rate decay. Worst case is reduce the jitter at end of training</li>\n",
    "<li>Weight initialization to small distribituion to keep output distribution uncertain</li>\n",
    "<li>images are normalized by subracting 128 and dividing by 128</li>\n",
    "<li>random sample the training set for each batch and calculate the average gradient</li>\n",
    "<li>use momentuim which is the running average of the gradients. Do not use the sampled gradient directly. Gradient\n",
    "= .9 *momentum + new sampled gradient </li>\n",
    "<li></li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__globals__': [], '__version__': '1.0', '__header__': b'MATLAB 5.0 MAT-file, written by Octave 3.2.4, 2012-10-31 21:15:39 UTC', 'data': <scipy.io.matlab.mio5_params.mat_struct object at 0x11f8d1e48>}\n",
      "<class 'scipy.io.matlab.mio5_params.mat_struct'>\n"
     ]
    }
   ],
   "source": [
    "def a3(wd_coefficient, n_hid, n_iters, learning_rate, momentum_multiplier, do_early_stopping, mini_batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "import scipy.io as sio\n",
    "import os\n",
    "\n",
    "def load_data():\n",
    "    path = \"/Users/dc/DeepLearning/hinton/assignment3\"\n",
    "    filename=\"data.mat\"\n",
    "    data = sio.loadmat(os.path.join(path,filename),struct_as_record=False, squeeze_me=True)\n",
    "    print (data)\n",
    "    data = data['data']\n",
    "    print (type(data))\n",
    "    #print (data.shape)\n",
    "    #print(data[0]['training'])\n",
    "    return data\n",
    "\n",
    "foo = load_data()\n",
    "#print(foo.dtype)\n",
    "#train = foo['training']\n",
    "#print (train.dtype)\n",
    "#print (train[0][0])\n",
    "#print (train[0][0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (train[0][0][0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
